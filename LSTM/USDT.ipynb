{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:38.409814Z",
     "start_time": "2025-03-21T16:51:34.287017Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:38.425547Z",
     "start_time": "2025-03-21T16:51:38.416753Z"
    }
   },
   "cell_type": "code",
   "source": "import data.data_retriever as data_retriever",
   "id": "5dcf505a2204266c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:38.622783Z",
     "start_time": "2025-03-21T16:51:38.609614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "set_seed(0)"
   ],
   "id": "bd311c5e1caaa1fd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:40.797463Z",
     "start_time": "2025-03-21T16:51:40.626734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = data_retriever.get_data('USDT')\n",
    "df['unix_time'] = pd.to_datetime(df['date-time']).astype(int) / 10**9\n",
    "\n",
    "cols_to_keep = [\n",
    "    'unix_time',\n",
    "    'liquidityRate_avg',\n",
    "    'variableBorrowRate_avg',\n",
    "    'utilizationRate_avg',\n",
    "    'close_price',\n",
    "    'volume',\n",
    "]\n",
    "\n",
    "crypto_df = df[cols_to_keep]\n",
    "crypto_df.head()"
   ],
   "id": "4dd14380c8a81907",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      unix_time  liquidityRate_avg  variableBorrowRate_avg  \\\n",
       "0  1.678752e+09           0.026841                0.033669   \n",
       "1  1.678756e+09           0.018498                0.023178   \n",
       "2  1.678759e+09           0.011905                0.014973   \n",
       "3  1.678763e+09           0.060175                0.076011   \n",
       "4  1.678766e+09           0.026422                0.034169   \n",
       "\n",
       "   utilizationRate_avg  close_price        volume  \n",
       "0             0.885642      1.00450  5.479494e+06  \n",
       "1             0.888167      1.00421  7.850737e+06  \n",
       "2             0.883226      1.00463  7.511133e+06  \n",
       "3             0.878935      1.00520  7.736502e+06  \n",
       "4             0.859224      1.00480  8.454853e+06  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unix_time</th>\n",
       "      <th>liquidityRate_avg</th>\n",
       "      <th>variableBorrowRate_avg</th>\n",
       "      <th>utilizationRate_avg</th>\n",
       "      <th>close_price</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.678752e+09</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>0.033669</td>\n",
       "      <td>0.885642</td>\n",
       "      <td>1.00450</td>\n",
       "      <td>5.479494e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.678756e+09</td>\n",
       "      <td>0.018498</td>\n",
       "      <td>0.023178</td>\n",
       "      <td>0.888167</td>\n",
       "      <td>1.00421</td>\n",
       "      <td>7.850737e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.678759e+09</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.014973</td>\n",
       "      <td>0.883226</td>\n",
       "      <td>1.00463</td>\n",
       "      <td>7.511133e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.678763e+09</td>\n",
       "      <td>0.060175</td>\n",
       "      <td>0.076011</td>\n",
       "      <td>0.878935</td>\n",
       "      <td>1.00520</td>\n",
       "      <td>7.736502e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.678766e+09</td>\n",
       "      <td>0.026422</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>0.859224</td>\n",
       "      <td>1.00480</td>\n",
       "      <td>8.454853e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataloader",
   "id": "56e04f6711da3368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:42.301358Z",
     "start_time": "2025-03-21T16:51:42.290276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, window_size: int, target_columns, forecast_size: int = 1):\n",
    "        self.window_size = window_size\n",
    "        self.forecast_size = forecast_size\n",
    "        self.target_columns = target_columns  \n",
    "        \n",
    "        data_array = data.values\n",
    "    \n",
    "        target_indices = [data.columns.get_loc(col) for col in target_columns]  \n",
    "\n",
    "        self.X, self.y = [], []\n",
    "        for i in range(len(data_array) - window_size - forecast_size + 1):\n",
    "            self.X.append(data_array[i : i + window_size])\n",
    "            \n",
    "            if len(target_indices) == 1:\n",
    "                self.y.append(data_array[i + window_size : i + window_size + forecast_size, target_indices[0]].reshape(-1, 1))\n",
    "            else:\n",
    "                self.y.append(data_array[i + window_size : i + window_size + forecast_size, target_indices])\n",
    "\n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.X[idx], dtype=torch.float32), \n",
    "                torch.tensor(self.y[idx], dtype=torch.float32))"
   ],
   "id": "a76bed50397e338",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:42.528295Z",
     "start_time": "2025-03-21T16:51:42.521307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloaders(\n",
    "        data, \n",
    "        target_columns,\n",
    "        window_size=168,\n",
    "        forecast_size=1,\n",
    "        batch_size=32, \n",
    "        val_size=0.1,\n",
    "        test_size=0.1):\n",
    "    total_size = len(data)\n",
    "    train_size = int((1 - val_size - test_size) * total_size)\n",
    "    val_size = int(val_size * total_size)\n",
    "    \n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(train_data, window_size, target_columns, forecast_size)\n",
    "    val_dataset = TimeSeriesDataset(val_data, window_size, target_columns, forecast_size)\n",
    "    test_dataset = TimeSeriesDataset(test_data, window_size, target_columns, forecast_size)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "id": "8b28f10830f5cb37",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "724b3cf2850f5e55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:42.911806Z",
     "start_time": "2025-03-21T16:51:42.902749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=2, forecast_size=1, dropout=0.2):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.forecast_size = forecast_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        \n",
    "        forecast = []\n",
    "        for _ in range(self.forecast_size):\n",
    "            out = self.fc(out)\n",
    "            forecast.append(out.unsqueeze(1))\n",
    "        \n",
    "        forecast = torch.cat(forecast, dim=1) \n",
    "        return forecast"
   ],
   "id": "71e6229e8f3f52ad",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Trainer",
   "id": "87e93ebb380ec9e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:43.630364Z",
     "start_time": "2025-03-21T16:51:43.612663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001, device=\"cuda\", patience=10, lr_patience=5, lr_decay_factor=0.1):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_factor, patience=lr_patience, verbose=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement_es = 0\n",
    "    epochs_without_improvement_lr = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "                y_pred = model(X_val)\n",
    "                loss = criterion(y_pred, y_val)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.8f}, Val Loss: {val_loss:.8f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement_es = 0\n",
    "            epochs_without_improvement_lr = 0\n",
    "        else:\n",
    "            epochs_without_improvement_es += 1\n",
    "            epochs_without_improvement_lr += 1\n",
    "\n",
    "        if epochs_without_improvement_es >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        if epochs_without_improvement_lr >= lr_patience:\n",
    "            print(f\"Validation loss plateaued for {lr_patience} epochs. Reducing learning rate by factor of {lr_decay_factor}.\")\n",
    "            scheduler.step(val_loss)\n",
    "            epochs_without_improvement_lr = 0"
   ],
   "id": "2e7d6c36baad6bc0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluator",
   "id": "e65b05a647451440"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:44.571629Z",
     "start_time": "2025-03-21T16:51:44.558163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_loader, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    total_samples = 0\n",
    "    total_mape = torch.zeros(model.output_size, device=device) \n",
    "    total_mse = torch.zeros(model.output_size, device=device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            y_pred = model(X_test)\n",
    "\n",
    "            mse_per_element = criterion(y_pred, y_test)\n",
    "            mape_per_output = torch.mean(torch.abs((y_test - y_pred) / (y_test + 1e-8)), dim=(0, 1)) * 100\n",
    "\n",
    "            total_mape += mape_per_output * X_test.size(0)\n",
    "            total_samples += X_test.size(0)\n",
    "\n",
    "            mse_per_output = mse_per_element.mean(dim=(0, 1)) \n",
    "            total_mse += mse_per_output * X_test.size(0)\n",
    "\n",
    "    average_mse = total_mse.cpu().numpy() / total_samples\n",
    "    average_mape = total_mape.cpu().numpy() / total_samples\n",
    "\n",
    "    print(f\"Test MSE (for each output dimension): {average_mse}\")\n",
    "    print(f\"Test MAPE (for each output dimension): {average_mape}\")\n",
    "\n",
    "    return average_mse, average_mape\n"
   ],
   "id": "bc2bb25c5f6e902d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d77b10f1f990d624"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "4a4de125add9bae2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:51:46.425937Z",
     "start_time": "2025-03-21T16:51:46.377346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(\n",
    "        crypto_df,\n",
    "        input_size=6, \n",
    "        hidden_size=32, \n",
    "        num_layers=4, \n",
    "        output_size=2, \n",
    "        forecast_size=1, \n",
    "        dropout=0.2,\n",
    "        target_columns=['liquidityRate_avg', 'variableBorrowRate_avg'],\n",
    "        window_size=168,\n",
    "        batch_size=32,\n",
    "        val_size=0.1,\n",
    "        test_size=0.1,\n",
    "        num_epochs=100,\n",
    "        learning_rate=0.001,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        patience=10,\n",
    "        lr_patience=5,\n",
    "        lr_decay_factor=0.1,\n",
    "):\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        crypto_df,\n",
    "        target_columns=target_columns,\n",
    "        window_size=window_size,\n",
    "        forecast_size=forecast_size,\n",
    "        batch_size=batch_size,\n",
    "        val_size=val_size,\n",
    "        test_size=test_size\n",
    "    )\n",
    "    \n",
    "    model = LSTMForecaster(\n",
    "        input_size, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        output_size, \n",
    "        forecast_size, \n",
    "        dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        num_epochs=num_epochs, \n",
    "        learning_rate=learning_rate, \n",
    "        device=device, \n",
    "        patience=patience, \n",
    "        lr_patience=lr_patience, \n",
    "        lr_decay_factor=lr_decay_factor\n",
    "    )\n",
    "\n",
    "    evaluate_model(model, test_loader, device=device)"
   ],
   "id": "428c9d057efc70cc",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:52:42.336943Z",
     "start_time": "2025-03-21T16:51:47.451692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# window size of 1 day\n",
    "train_and_evaluate(crypto_df, window_size=24)"
   ],
   "id": "b94f35141283a091",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siam\\anaconda3\\envs\\aave\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.00191607, Val Loss: 0.00230264\n",
      "Epoch 2/100, Train Loss: 0.00186930, Val Loss: 0.00235975\n",
      "Epoch 3/100, Train Loss: 0.00186489, Val Loss: 0.00218279\n",
      "Epoch 4/100, Train Loss: 0.00186761, Val Loss: 0.00220394\n",
      "Epoch 5/100, Train Loss: 0.00185475, Val Loss: 0.00235606\n",
      "Epoch 6/100, Train Loss: 0.00185826, Val Loss: 0.00222995\n",
      "Epoch 7/100, Train Loss: 0.00185403, Val Loss: 0.00223126\n",
      "Epoch 8/100, Train Loss: 0.00185839, Val Loss: 0.00225587\n",
      "Validation loss plateaued for 5 epochs. Reducing learning rate by factor of 0.1.\n",
      "Epoch 9/100, Train Loss: 0.00185263, Val Loss: 0.00223713\n",
      "Epoch 10/100, Train Loss: 0.00185801, Val Loss: 0.00224372\n",
      "Epoch 11/100, Train Loss: 0.00185463, Val Loss: 0.00238382\n",
      "Epoch 12/100, Train Loss: 0.00185006, Val Loss: 0.00223432\n",
      "Epoch 13/100, Train Loss: 0.00185138, Val Loss: 0.00230541\n",
      "Early stopping at epoch 13 due to no improvement in validation loss for 10 epochs.\n",
      "Test MSE (for each output dimension): [0.00044847 0.00088729]\n",
      "Test MAPE (for each output dimension): [31.2075   26.116764]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:53:28.661475Z",
     "start_time": "2025-03-21T16:52:48.162352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# window size of 1 week\n",
    "train_and_evaluate(crypto_df, window_size=7*24)"
   ],
   "id": "2afa4048f8e4b9d1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siam\\anaconda3\\envs\\aave\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.00213524, Val Loss: 0.00228686\n",
      "Epoch 2/100, Train Loss: 0.00188656, Val Loss: 0.00233357\n",
      "Epoch 3/100, Train Loss: 0.00187718, Val Loss: 0.00257181\n",
      "Epoch 4/100, Train Loss: 0.00188997, Val Loss: 0.00225524\n",
      "Epoch 5/100, Train Loss: 0.00187924, Val Loss: 0.00249794\n",
      "Epoch 6/100, Train Loss: 0.00186706, Val Loss: 0.00246717\n",
      "Epoch 7/100, Train Loss: 0.00186754, Val Loss: 0.00239406\n",
      "Epoch 8/100, Train Loss: 0.00186503, Val Loss: 0.00242032\n",
      "Epoch 9/100, Train Loss: 0.00186556, Val Loss: 0.00261359\n",
      "Validation loss plateaued for 5 epochs. Reducing learning rate by factor of 0.1.\n",
      "Epoch 10/100, Train Loss: 0.00186986, Val Loss: 0.00241719\n",
      "Epoch 11/100, Train Loss: 0.00186108, Val Loss: 0.00243264\n",
      "Epoch 12/100, Train Loss: 0.00186482, Val Loss: 0.00247978\n",
      "Epoch 13/100, Train Loss: 0.00186504, Val Loss: 0.00235851\n",
      "Epoch 14/100, Train Loss: 0.00186639, Val Loss: 0.00253142\n",
      "Early stopping at epoch 14 due to no improvement in validation loss for 10 epochs.\n",
      "Test MSE (for each output dimension): [0.00040975 0.00088316]\n",
      "Test MAPE (for each output dimension): [30.822256 26.24103 ]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:55:05.039301Z",
     "start_time": "2025-03-21T16:53:31.889188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# window size of 1 month\n",
    "train_and_evaluate(crypto_df, window_size=30*24)"
   ],
   "id": "cb81f507cd524a43",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siam\\anaconda3\\envs\\aave\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.00216622, Val Loss: 0.00327930\n",
      "Epoch 2/100, Train Loss: 0.00189674, Val Loss: 0.00373778\n",
      "Epoch 3/100, Train Loss: 0.00189325, Val Loss: 0.00321490\n",
      "Epoch 4/100, Train Loss: 0.00189153, Val Loss: 0.00389923\n",
      "Epoch 5/100, Train Loss: 0.00189564, Val Loss: 0.00332547\n",
      "Epoch 6/100, Train Loss: 0.00190431, Val Loss: 0.00318835\n",
      "Epoch 7/100, Train Loss: 0.00188726, Val Loss: 0.00350815\n",
      "Epoch 8/100, Train Loss: 0.00188963, Val Loss: 0.00362196\n",
      "Epoch 9/100, Train Loss: 0.00188924, Val Loss: 0.00336556\n",
      "Epoch 10/100, Train Loss: 0.00189732, Val Loss: 0.00362094\n",
      "Epoch 11/100, Train Loss: 0.00188465, Val Loss: 0.00367605\n",
      "Validation loss plateaued for 5 epochs. Reducing learning rate by factor of 0.1.\n",
      "Epoch 12/100, Train Loss: 0.00189110, Val Loss: 0.00357516\n",
      "Epoch 13/100, Train Loss: 0.00188780, Val Loss: 0.00362915\n",
      "Epoch 14/100, Train Loss: 0.00188540, Val Loss: 0.00360895\n",
      "Epoch 15/100, Train Loss: 0.00188739, Val Loss: 0.00340196\n",
      "Epoch 16/100, Train Loss: 0.00188637, Val Loss: 0.00352969\n",
      "Early stopping at epoch 16 due to no improvement in validation loss for 10 epochs.\n",
      "Test MSE (for each output dimension): [0.00021368 0.00038748]\n",
      "Test MAPE (for each output dimension): [32.97227 20.15092]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fb9cd3336100d674"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
