{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T16:32:59.177280Z",
     "start_time": "2025-03-21T16:32:59.166232Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:32:59.595018Z",
     "start_time": "2025-03-21T16:32:59.589867Z"
    }
   },
   "cell_type": "code",
   "source": "import data.data_retriever as data_retriever",
   "id": "5dcf505a2204266c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:33:00.346034Z",
     "start_time": "2025-03-21T16:33:00.337954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "set_seed(0)"
   ],
   "id": "bd311c5e1caaa1fd",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:33:02.288531Z",
     "start_time": "2025-03-21T16:33:01.104916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = data_retriever.get_data('ETH')\n",
    "df['unix_time'] = pd.to_datetime(df['date-time']).astype(int) / 10**9\n",
    "\n",
    "cols_to_keep = [\n",
    "    'unix_time',\n",
    "    'liquidityRate_avg',\n",
    "    'variableBorrowRate_avg',\n",
    "    'utilizationRate_avg',\n",
    "    'close_price',\n",
    "    'volume',\n",
    "]\n",
    "\n",
    "crypto_df = df[cols_to_keep]\n",
    "crypto_df.head()"
   ],
   "id": "4dd14380c8a81907",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      unix_time  liquidityRate_avg  variableBorrowRate_avg  \\\n",
       "0  1.678752e+09           0.027728                0.043863   \n",
       "1  1.678756e+09           0.020744                0.032873   \n",
       "2  1.678759e+09           0.031029                0.049285   \n",
       "3  1.678763e+09           0.027092                0.043039   \n",
       "4  1.678766e+09           0.028781                0.045741   \n",
       "\n",
       "   utilizationRate_avg  close_price        volume  \n",
       "0             0.743329      1676.49   8843.165993  \n",
       "1             0.741720      1684.56   6320.830728  \n",
       "2             0.740579      1676.25  10817.093686  \n",
       "3             0.740210      1686.02   9007.115071  \n",
       "4             0.739932      1685.06  13505.217976  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unix_time</th>\n",
       "      <th>liquidityRate_avg</th>\n",
       "      <th>variableBorrowRate_avg</th>\n",
       "      <th>utilizationRate_avg</th>\n",
       "      <th>close_price</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.678752e+09</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.043863</td>\n",
       "      <td>0.743329</td>\n",
       "      <td>1676.49</td>\n",
       "      <td>8843.165993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.678756e+09</td>\n",
       "      <td>0.020744</td>\n",
       "      <td>0.032873</td>\n",
       "      <td>0.741720</td>\n",
       "      <td>1684.56</td>\n",
       "      <td>6320.830728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.678759e+09</td>\n",
       "      <td>0.031029</td>\n",
       "      <td>0.049285</td>\n",
       "      <td>0.740579</td>\n",
       "      <td>1676.25</td>\n",
       "      <td>10817.093686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.678763e+09</td>\n",
       "      <td>0.027092</td>\n",
       "      <td>0.043039</td>\n",
       "      <td>0.740210</td>\n",
       "      <td>1686.02</td>\n",
       "      <td>9007.115071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.678766e+09</td>\n",
       "      <td>0.028781</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.739932</td>\n",
       "      <td>1685.06</td>\n",
       "      <td>13505.217976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataloader",
   "id": "56e04f6711da3368"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:33:04.257301Z",
     "start_time": "2025-03-21T16:33:04.245666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, window_size: int, target_columns, forecast_size: int = 1):\n",
    "        self.window_size = window_size\n",
    "        self.forecast_size = forecast_size\n",
    "        self.target_columns = target_columns  \n",
    "        \n",
    "        data_array = data.values\n",
    "    \n",
    "        target_indices = [data.columns.get_loc(col) for col in target_columns]  \n",
    "\n",
    "        self.X, self.y = [], []\n",
    "        for i in range(len(data_array) - window_size - forecast_size + 1):\n",
    "            self.X.append(data_array[i : i + window_size])\n",
    "            \n",
    "            if len(target_indices) == 1:\n",
    "                self.y.append(data_array[i + window_size : i + window_size + forecast_size, target_indices[0]].reshape(-1, 1))\n",
    "            else:\n",
    "                self.y.append(data_array[i + window_size : i + window_size + forecast_size, target_indices])\n",
    "\n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.X[idx], dtype=torch.float32), \n",
    "                torch.tensor(self.y[idx], dtype=torch.float32))"
   ],
   "id": "a76bed50397e338",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:33:04.894148Z",
     "start_time": "2025-03-21T16:33:04.883502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloaders(\n",
    "        data, \n",
    "        target_columns,\n",
    "        window_size=168,\n",
    "        forecast_size=1,\n",
    "        batch_size=32, \n",
    "        val_size=0.1,\n",
    "        test_size=0.1):\n",
    "    total_size = len(data)\n",
    "    train_size = int((1 - val_size - test_size) * total_size)\n",
    "    val_size = int(val_size * total_size)\n",
    "    \n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(train_data, window_size, target_columns, forecast_size)\n",
    "    val_dataset = TimeSeriesDataset(val_data, window_size, target_columns, forecast_size)\n",
    "    test_dataset = TimeSeriesDataset(test_data, window_size, target_columns, forecast_size)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "id": "8b28f10830f5cb37",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "724b3cf2850f5e55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:33:06.149116Z",
     "start_time": "2025-03-21T16:33:06.137865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=2, forecast_size=1, dropout=0.2):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.forecast_size = forecast_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        \n",
    "        forecast = []\n",
    "        for _ in range(self.forecast_size):\n",
    "            out = self.fc(out)\n",
    "            forecast.append(out.unsqueeze(1))\n",
    "        \n",
    "        forecast = torch.cat(forecast, dim=1) \n",
    "        return forecast"
   ],
   "id": "71e6229e8f3f52ad",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Trainer",
   "id": "87e93ebb380ec9e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:33:08.057430Z",
     "start_time": "2025-03-21T16:33:08.043766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001, device=\"cuda\", patience=10, lr_patience=5, lr_decay_factor=0.1):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay_factor, patience=lr_patience, verbose=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_without_improvement_es = 0\n",
    "    epochs_without_improvement_lr = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "                y_pred = model(X_val)\n",
    "                loss = criterion(y_pred, y_val)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.8f}, Val Loss: {val_loss:.8f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement_es = 0\n",
    "            epochs_without_improvement_lr = 0\n",
    "        else:\n",
    "            epochs_without_improvement_es += 1\n",
    "            epochs_without_improvement_lr += 1\n",
    "\n",
    "        if epochs_without_improvement_es >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        if epochs_without_improvement_lr >= lr_patience:\n",
    "            print(f\"Validation loss plateaued for {lr_patience} epochs. Reducing learning rate by factor of {lr_decay_factor}.\")\n",
    "            scheduler.step(val_loss)\n",
    "            epochs_without_improvement_lr = 0"
   ],
   "id": "2e7d6c36baad6bc0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluator",
   "id": "e65b05a647451440"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:33:09.462214Z",
     "start_time": "2025-03-21T16:33:09.448472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_loader, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    total_samples = 0\n",
    "    total_mape = torch.zeros(model.output_size, device=device) \n",
    "    total_mse = torch.zeros(model.output_size, device=device) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            y_pred = model(X_test)\n",
    "\n",
    "            mse_per_element = criterion(y_pred, y_test)\n",
    "            mape_per_output = torch.mean(torch.abs((y_test - y_pred) / (y_test + 1e-8)), dim=(0, 1)) * 100\n",
    "\n",
    "            total_mape += mape_per_output * X_test.size(0)\n",
    "            total_samples += X_test.size(0)\n",
    "\n",
    "            mse_per_output = mse_per_element.mean(dim=(0, 1)) \n",
    "            total_mse += mse_per_output * X_test.size(0)\n",
    "\n",
    "    average_mse = total_mse.cpu().numpy() / total_samples\n",
    "    average_mape = total_mape.cpu().numpy() / total_samples\n",
    "\n",
    "    print(f\"Test MSE (for each output dimension): {average_mse}\")\n",
    "    print(f\"Test MAPE (for each output dimension): {average_mape}\")\n",
    "\n",
    "    return average_mse, average_mape\n"
   ],
   "id": "bc2bb25c5f6e902d",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d77b10f1f990d624"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "4a4de125add9bae2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:33:11.313387Z",
     "start_time": "2025-03-21T16:33:11.303962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(\n",
    "        crypto_df,\n",
    "        input_size=6, \n",
    "        hidden_size=32, \n",
    "        num_layers=4, \n",
    "        output_size=2, \n",
    "        forecast_size=1, \n",
    "        dropout=0.2,\n",
    "        target_columns=['liquidityRate_avg', 'variableBorrowRate_avg'],\n",
    "        window_size=168,\n",
    "        batch_size=32,\n",
    "        val_size=0.1,\n",
    "        test_size=0.1,\n",
    "        num_epochs=100,\n",
    "        learning_rate=0.001,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        patience=10,\n",
    "        lr_patience=5,\n",
    "        lr_decay_factor=0.1,\n",
    "):\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        crypto_df,\n",
    "        target_columns=target_columns,\n",
    "        window_size=window_size,\n",
    "        forecast_size=forecast_size,\n",
    "        batch_size=batch_size,\n",
    "        val_size=val_size,\n",
    "        test_size=test_size\n",
    "    )\n",
    "    \n",
    "    model = LSTMForecaster(\n",
    "        input_size, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        output_size, \n",
    "        forecast_size, \n",
    "        dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        num_epochs=num_epochs, \n",
    "        learning_rate=learning_rate, \n",
    "        device=device, \n",
    "        patience=patience, \n",
    "        lr_patience=lr_patience, \n",
    "        lr_decay_factor=lr_decay_factor\n",
    "    )\n",
    "\n",
    "    evaluate_model(model, test_loader, device=device)"
   ],
   "id": "428c9d057efc70cc",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:35:21.990696Z",
     "start_time": "2025-03-21T16:34:05.762574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# window size of 1 day\n",
    "train_and_evaluate(crypto_df, window_size=24)"
   ],
   "id": "b94f35141283a091",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siam\\anaconda3\\envs\\aave\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.00045799, Val Loss: 0.00005818\n",
      "Epoch 2/100, Train Loss: 0.00034283, Val Loss: 0.00004138\n",
      "Epoch 3/100, Train Loss: 0.00034078, Val Loss: 0.00003982\n",
      "Epoch 4/100, Train Loss: 0.00034113, Val Loss: 0.00004356\n",
      "Epoch 5/100, Train Loss: 0.00033988, Val Loss: 0.00003934\n",
      "Epoch 6/100, Train Loss: 0.00033923, Val Loss: 0.00004698\n",
      "Epoch 7/100, Train Loss: 0.00033934, Val Loss: 0.00004278\n",
      "Epoch 8/100, Train Loss: 0.00033851, Val Loss: 0.00003990\n",
      "Epoch 9/100, Train Loss: 0.00033890, Val Loss: 0.00005589\n",
      "Epoch 10/100, Train Loss: 0.00033863, Val Loss: 0.00006324\n",
      "Validation loss plateaued for 5 epochs. Reducing learning rate by factor of 0.1.\n",
      "Epoch 11/100, Train Loss: 0.00033897, Val Loss: 0.00004047\n",
      "Epoch 12/100, Train Loss: 0.00033833, Val Loss: 0.00004401\n",
      "Epoch 13/100, Train Loss: 0.00033797, Val Loss: 0.00005095\n",
      "Epoch 14/100, Train Loss: 0.00033800, Val Loss: 0.00004520\n",
      "Epoch 15/100, Train Loss: 0.00033813, Val Loss: 0.00003779\n",
      "Epoch 16/100, Train Loss: 0.00033778, Val Loss: 0.00004301\n",
      "Epoch 17/100, Train Loss: 0.00033794, Val Loss: 0.00004740\n",
      "Epoch 18/100, Train Loss: 0.00033749, Val Loss: 0.00003925\n",
      "Epoch 19/100, Train Loss: 0.00033756, Val Loss: 0.00005711\n",
      "Epoch 20/100, Train Loss: 0.00033795, Val Loss: 0.00004938\n",
      "Validation loss plateaued for 5 epochs. Reducing learning rate by factor of 0.1.\n",
      "Epoch 21/100, Train Loss: 0.00033681, Val Loss: 0.00004726\n",
      "Epoch 22/100, Train Loss: 0.00033762, Val Loss: 0.00004168\n",
      "Epoch 23/100, Train Loss: 0.00033750, Val Loss: 0.00004299\n",
      "Epoch 24/100, Train Loss: 0.00033670, Val Loss: 0.00004090\n",
      "Epoch 25/100, Train Loss: 0.00033694, Val Loss: 0.00004780\n",
      "Early stopping at epoch 25 due to no improvement in validation loss for 10 epochs.\n",
      "Test MSE (for each output dimension): [1.0383713e-05 5.1927535e-05]\n",
      "Test MAPE (for each output dimension): [14.142021 29.75045 ]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:37:55.050785Z",
     "start_time": "2025-03-21T16:37:00.465396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# window size of 1 week\n",
    "train_and_evaluate(crypto_df, window_size=7*24)"
   ],
   "id": "2afa4048f8e4b9d1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siam\\anaconda3\\envs\\aave\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.00037742, Val Loss: 0.00004782\n",
      "Epoch 2/100, Train Loss: 0.00034429, Val Loss: 0.00005757\n",
      "Epoch 3/100, Train Loss: 0.00034213, Val Loss: 0.00004439\n",
      "Epoch 4/100, Train Loss: 0.00034258, Val Loss: 0.00004446\n",
      "Epoch 5/100, Train Loss: 0.00034127, Val Loss: 0.00004324\n",
      "Epoch 6/100, Train Loss: 0.00034071, Val Loss: 0.00004261\n",
      "Epoch 7/100, Train Loss: 0.00034089, Val Loss: 0.00004267\n",
      "Epoch 8/100, Train Loss: 0.00034108, Val Loss: 0.00003960\n",
      "Epoch 9/100, Train Loss: 0.00034084, Val Loss: 0.00004578\n",
      "Epoch 10/100, Train Loss: 0.00034026, Val Loss: 0.00004300\n",
      "Epoch 11/100, Train Loss: 0.00033959, Val Loss: 0.00004292\n",
      "Epoch 12/100, Train Loss: 0.00034080, Val Loss: 0.00004086\n",
      "Epoch 13/100, Train Loss: 0.00034091, Val Loss: 0.00004366\n",
      "Validation loss plateaued for 5 epochs. Reducing learning rate by factor of 0.1.\n",
      "Epoch 14/100, Train Loss: 0.00033950, Val Loss: 0.00004662\n",
      "Epoch 15/100, Train Loss: 0.00033951, Val Loss: 0.00004370\n",
      "Epoch 16/100, Train Loss: 0.00033929, Val Loss: 0.00004496\n",
      "Epoch 17/100, Train Loss: 0.00033953, Val Loss: 0.00004791\n",
      "Epoch 18/100, Train Loss: 0.00033960, Val Loss: 0.00004310\n",
      "Early stopping at epoch 18 due to no improvement in validation loss for 10 epochs.\n",
      "Test MSE (for each output dimension): [1.11647605e-05 2.86249469e-05]\n",
      "Test MAPE (for each output dimension): [14.123555 20.213531]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T16:40:30.318445Z",
     "start_time": "2025-03-21T16:38:06.737235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# window size of 1 month\n",
    "train_and_evaluate(crypto_df, window_size=30*24)"
   ],
   "id": "cb81f507cd524a43",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siam\\anaconda3\\envs\\aave\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.00050464, Val Loss: 0.00005839\n",
      "Epoch 2/100, Train Loss: 0.00035216, Val Loss: 0.00005757\n",
      "Epoch 3/100, Train Loss: 0.00035008, Val Loss: 0.00005679\n",
      "Epoch 4/100, Train Loss: 0.00034762, Val Loss: 0.00005045\n",
      "Epoch 5/100, Train Loss: 0.00034969, Val Loss: 0.00004911\n",
      "Epoch 6/100, Train Loss: 0.00034705, Val Loss: 0.00004802\n",
      "Epoch 7/100, Train Loss: 0.00034642, Val Loss: 0.00004838\n",
      "Epoch 8/100, Train Loss: 0.00034740, Val Loss: 0.00007555\n",
      "Epoch 9/100, Train Loss: 0.00034672, Val Loss: 0.00004905\n",
      "Epoch 10/100, Train Loss: 0.00034743, Val Loss: 0.00005016\n",
      "Epoch 11/100, Train Loss: 0.00034617, Val Loss: 0.00005101\n",
      "Validation loss plateaued for 5 epochs. Reducing learning rate by factor of 0.1.\n",
      "Epoch 12/100, Train Loss: 0.00034712, Val Loss: 0.00005048\n",
      "Epoch 13/100, Train Loss: 0.00034606, Val Loss: 0.00004625\n",
      "Epoch 14/100, Train Loss: 0.00034427, Val Loss: 0.00009446\n",
      "Epoch 15/100, Train Loss: 0.00034908, Val Loss: 0.00004798\n",
      "Epoch 16/100, Train Loss: 0.00034689, Val Loss: 0.00004599\n",
      "Epoch 17/100, Train Loss: 0.00034553, Val Loss: 0.00005129\n",
      "Epoch 18/100, Train Loss: 0.00034549, Val Loss: 0.00005177\n",
      "Epoch 19/100, Train Loss: 0.00034589, Val Loss: 0.00004704\n",
      "Epoch 20/100, Train Loss: 0.00034540, Val Loss: 0.00004725\n",
      "Epoch 21/100, Train Loss: 0.00034537, Val Loss: 0.00004945\n",
      "Validation loss plateaued for 5 epochs. Reducing learning rate by factor of 0.1.\n",
      "Epoch 22/100, Train Loss: 0.00034614, Val Loss: 0.00006107\n",
      "Epoch 23/100, Train Loss: 0.00034532, Val Loss: 0.00004882\n",
      "Epoch 24/100, Train Loss: 0.00034544, Val Loss: 0.00005103\n",
      "Epoch 25/100, Train Loss: 0.00034574, Val Loss: 0.00004837\n",
      "Epoch 26/100, Train Loss: 0.00034540, Val Loss: 0.00004876\n",
      "Early stopping at epoch 26 due to no improvement in validation loss for 10 epochs.\n",
      "Test MSE (for each output dimension): [1.0655504e-05 3.1529686e-05]\n",
      "Test MAPE (for each output dimension): [12.341257 20.833075]\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
